{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to make a model to determine if a flight will be cancelled or not. To try to amplify the signal, we'll focus on just the flights that were cancelled due to weather, and flights that occured in poor weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data from the previous notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "flights = pd.read_csv('flights_with_wx.csv', \n",
    "                      dtype= {'DAY_OF_WEEK': 'uint8',\n",
    "                             'AIRLINE': 'category',\n",
    "                             'ORIGIN_AIRPORT': 'category',\n",
    "                             'DESTINATION_AIRPORT': 'category',\n",
    "                             'DEPARTURE_DELAY': 'float32',\n",
    "                             'ARRIVAL_DELAY': 'float32',\n",
    "                             'DIVERTED': 'uint8',\n",
    "                             'CANCELLED': 'uint8',\n",
    "                             'CANCELLATION_REASON': 'category',\n",
    "                             'ORIGIN_CEILING': 'uint16', \n",
    "                             'ORIGIN_VISIBILITY': 'float16', \n",
    "                             'ORIGIN_WIND_SPEED': 'float16',\n",
    "                             'ORIGIN_PRECIPITATION': 'float32', \n",
    "                             'DESTINATION_CEILING': 'uint16', \n",
    "                             'DESTINATION_VISIBILITY': 'float16',\n",
    "                             'DESTINATION_WIND_SPEED': 'float16', \n",
    "                             'DESTINATION_PRECIPITATION': 'float32'}, \n",
    "                      parse_dates=['SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL'])\n",
    "flights.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "flights_with_wxcx = flights[(flights['CANCELLATION_REASON'] != 'A') & (flights['CANCELLATION_REASON'] != 'C') & (flights['CANCELLATION_REASON'] != 'D')]\n",
    "flights_with_wxcx.drop('CANCELLATION_REASON', axis=1, inplace=True)\n",
    "\n",
    "flights_with_wxcx['ORIGIN_VISIBILITY'].fillna(10, inplace=True)\n",
    "flights_with_wxcx['ORIGIN_WIND_SPEED'].fillna(7, inplace=True)\n",
    "flights_with_wxcx['ORIGIN_PRECIPITATION'].fillna(0, inplace=True)\n",
    "flights_with_wxcx['DESTINATION_VISIBILITY'].fillna(10, inplace=True)\n",
    "flights_with_wxcx['DESTINATION_WIND_SPEED'].fillna(7, inplace=True)\n",
    "flights_with_wxcx['DESTINATION_PRECIPITATION'].fillna(0, inplace=True)\n",
    "flights_with_wxcx.loc[flights_with_wxcx['ORIGIN_CEILING'] > 25000, 'ORIGIN_CEILING'] = 25000\n",
    "flights_with_wxcx.loc[flights_with_wxcx['DESTINATION_CEILING'] > 25000, 'DESTINATION_CEILING'] = 25000\n",
    "\n",
    "flights_with_wxcx = flights_with_wxcx[\n",
    "    (flights_with_wxcx['ORIGIN_CEILING'] < 5000) |\n",
    "    (flights_with_wxcx['ORIGIN_VISIBILITY'] < 5) |\n",
    "    (flights_with_wxcx['ORIGIN_WIND_SPEED'] > 20) |\n",
    "    (flights_with_wxcx['DESTINATION_CEILING'] < 5000) |\n",
    "    (flights_with_wxcx['DESTINATION_VISIBILITY'] < 5) |\n",
    "    (flights_with_wxcx['DESTINATION_WIND_SPEED'] > 20)].dropna(\n",
    "    subset=['DESTINATION_CEILING', 'DESTINATION_VISIBILITY', 'DESTINATION_WIND_SPEED'])\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by splitting the data into a training subset and a testing subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(flights_with_wxcx[['ORIGIN_CEILING', \n",
    "                                                                       'ORIGIN_VISIBILITY',\n",
    "                                                                       'ORIGIN_PRECIPITATION',\n",
    "                                                                       'DESTINATION_CEILING',\n",
    "                                                                       'DESTINATION_VISIBILITY',\n",
    "                                                                       'DESTINATION_PRECIPITATION']],\n",
    "                                                    flights_with_wxcx['CANCELLED'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a basic logistic classification model and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    677373\n",
      "           1       0.00      0.00      0.00     10597\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    687970\n",
      "   macro avg       0.49      0.50      0.50    687970\n",
      "weighted avg       0.97      0.98      0.98    687970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, that was pretty poor. Random Forests might do a little better with our weak signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    677373\n",
      "           1       0.37      0.09      0.15     10597\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    687970\n",
      "   macro avg       0.68      0.54      0.57    687970\n",
      "weighted avg       0.98      0.98      0.98    687970\n",
      "\n",
      "[[675711   1662]\n",
      " [  9631    966]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators=1500, n_jobs=-1, verbose=0).fit(X_train, y_train)\n",
    "y_pred_rfc = rfc_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_rfc))\n",
    "print(confusion_matrix(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but still not great. Let's add a few more columns of data and see if that helps. We'll need to one-hot encode our catagorical variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_encoded = pd.concat([\n",
    "    pd.get_dummies(flights_with_wxcx[['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']]), \n",
    "    flights_with_wxcx.drop(['SCHEDULED_DEPARTURE', 'AIRLINE', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_ARRIVAL', 'DEPARTURE_DELAY', 'DIVERTED', 'ARRIVAL_DELAY'], axis=1)],\n",
    "    axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(flights_encoded.drop('CANCELLED', axis=1),\n",
    "                                                    flights_encoded['CANCELLED'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    677373\n",
      "           1       0.84      0.11      0.19     10597\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    687970\n",
      "   macro avg       0.91      0.55      0.59    687970\n",
      "weighted avg       0.98      0.99      0.98    687970\n",
      "\n",
      "[[677153    220]\n",
      " [  9448   1149]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=0).fit(X_train, y_train)\n",
    "y_pred_rfc = rfc_model.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_rfc))\n",
    "print(confusion_matrix(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually isn't bad! To explain these results in plain english: only 11% of the flights that were actually cancelled were predicted to be cancelled by this Random Forest model. However, 84% of the flights that the model predicted to be cancelled were actually cancelled. If this model were to be used operationally, you would not rely on it as the sole source for predicting whether or not a flight would be cancelled, since it fails to catch 89% of flights that are cancelled. However, whenever it does predict a particular flight to be cancelled, you could rely on it to be correct and start implementing contingency solutions for getting passengers to their destination. \n",
    "\n",
    "We could potentially increase the precison and recall of this model with more estimators, but that would require significantly more processing power than my laptop can provide! Similarly, a boosted method like AdaBoost or Gradient Tree Boosting could produce better results, but their inherent sequential nature does not allow for parallel computation, making them time-consuming to train on a laptop.\n",
    "\n",
    "Lastly for our cancellation modeling, let's try a Deep Neural Network with TensorFlow. We'll start by scaling our numerical feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = flights_with_wxcx[['AIRLINE', \n",
    "                          'ORIGIN_AIRPORT',\n",
    "                          'DESTINATION_AIRPORT',\n",
    "                          'ORIGIN_CEILING', \n",
    "                          'ORIGIN_VISIBILITY', \n",
    "                          'ORIGIN_WIND_SPEED',\n",
    "                          'DESTINATION_CEILING',\n",
    "                          'DESTINATION_VISIBILITY',\n",
    "                          'DESTINATION_WIND_SPEED',\n",
    "                          'CANCELLED']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data.drop(['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'CANCELLED'], axis=1))\n",
    "scaled_features = scaler.fit_transform(data.drop(['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'CANCELLED'], axis=1))\n",
    "df_feat = pd.concat([pd.DataFrame(scaled_features,columns=data.columns[3:-1]), \n",
    "                     flights_with_wxcx['AIRLINE'].reset_index(drop=True).astype(str),\n",
    "                     flights_with_wxcx['ORIGIN_AIRPORT'].reset_index(drop=True).astype(str),\n",
    "                     flights_with_wxcx['DESTINATION_AIRPORT'].reset_index(drop=True).astype(str)], \n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build our feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity('FATAL')\n",
    "\n",
    "origin_ceiling = tf.feature_column.numeric_column(\"ORIGIN_CEILING\")\n",
    "origin_vis = tf.feature_column.numeric_column('ORIGIN_VISIBILITY')\n",
    "origin_wind = tf.feature_column.numeric_column('ORIGIN_WIND_SPEED')\n",
    "dest_ceiling =tf.feature_column.numeric_column('DESTINATION_CEILING')\n",
    "dest_vis = tf.feature_column.numeric_column('DESTINATION_VISIBILITY')\n",
    "dest_wind = tf.feature_column.numeric_column('DESTINATION_WIND_SPEED')\n",
    "airline = tf.feature_column.categorical_column_with_vocabulary_list(key='AIRLINE', vocabulary_list=flights_with_wxcx['AIRLINE'].unique().astype(str).tolist())\n",
    "origin_airport = tf.feature_column.categorical_column_with_vocabulary_list(key='ORIGIN_AIRPORT', vocabulary_list=flights_with_wxcx['ORIGIN_AIRPORT'].unique().astype(str).tolist())\n",
    "dest_airport = tf.feature_column.categorical_column_with_vocabulary_list(key='DESTINATION_AIRPORT', vocabulary_list=flights_with_wxcx['DESTINATION_AIRPORT'].unique().astype(str).tolist())\n",
    "\n",
    "feat_cols = [origin_ceiling, \n",
    "             origin_vis, \n",
    "             origin_wind, \n",
    "             dest_ceiling, \n",
    "             dest_vis, \n",
    "             dest_wind, \n",
    "             tf.feature_column.indicator_column(airline), \n",
    "             tf.feature_column.indicator_column(origin_airport), \n",
    "             tf.feature_column.indicator_column(dest_airport)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll do another train_test_split with our selected columns, create the model, train it, and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ravis\\AppData\\Local\\Temp\\tmpn9it9m3_\n",
      "[[681747      0]\n",
      " [  6223      0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    681747\n",
      "           1       0.00      0.00      0.00      6223\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    687970\n",
      "   macro avg       0.50      0.50      0.50    687970\n",
      "weighted avg       0.98      0.99      0.99    687970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, data['CANCELLED'].reset_index(drop=True), test_size=0.33, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "dnn_classifier = tf.estimator.DNNClassifier(hidden_units=[10, 20, 10], n_classes=2,feature_columns=feat_cols)\n",
    "# We use an unsually large batch size here. Because the percent of cancelled flights is so low, we want to \n",
    "# try to ensure each batch contains at least one cancelled flight to avoid underfitting.\n",
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=100, shuffle=True)\n",
    "dnn_classifier.train(input_fn=input_func, steps=100000)\n",
    "\n",
    "pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test,batch_size=len(X_test),shuffle=False)\n",
    "dnn_flight_predictions = list(dnn_classifier.predict(input_fn=pred_fn))\n",
    "final_preds  = []\n",
    "for pred in dnn_flight_predictions:\n",
    "    final_preds.append(pred['class_ids'][0])\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,final_preds))\n",
    "print(classification_report(y_test,final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to play around with the number of hidden layers and the number of neurons per layer, but we'll stick with the Random Forest model and see if we can improve it with more training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try XGBoost and compare it to our Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91    677373\n",
      "           1       0.07      0.76      0.12     10597\n",
      "\n",
      "   micro avg       0.84      0.84      0.84    687970\n",
      "   macro avg       0.53      0.80      0.52    687970\n",
      "weighted avg       0.98      0.84      0.90    687970\n",
      "\n",
      "[[566486 110887]\n",
      " [  2564   8033]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=6, n_estimators=100, scale_pos_weight=(1/y_train.mean()), n_jobs=4).fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99    677373\n",
      "           1       0.27      0.40      0.32     10597\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    687970\n",
      "   macro avg       0.63      0.69      0.65    687970\n",
      "weighted avg       0.98      0.97      0.98    687970\n",
      "\n",
      "[[665933  11440]\n",
      " [  6387   4210]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=6, n_estimators=150, n_jobs=4, scale_pos_weight=10).fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    677373\n",
      "           1       0.34      0.44      0.38     10597\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    687970\n",
      "   macro avg       0.67      0.71      0.69    687970\n",
      "weighted avg       0.98      0.98      0.98    687970\n",
      "\n",
      "[[668442   8931]\n",
      " [  5969   4628]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=10, n_estimators=150, n_jobs=4, scale_pos_weight=10).fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    677373\n",
      "           1       0.67      0.26      0.37     10597\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    687970\n",
      "   macro avg       0.83      0.63      0.68    687970\n",
      "weighted avg       0.98      0.99      0.98    687970\n",
      "\n",
      "[[676039   1334]\n",
      " [  7850   2747]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=10, n_estimators=400, n_jobs=4, scale_pos_weight=2).fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    677373\n",
      "           1       0.70      0.28      0.40     10597\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    687970\n",
      "   macro avg       0.84      0.64      0.70    687970\n",
      "weighted avg       0.98      0.99      0.98    687970\n",
      "\n",
      "[[676067   1306]\n",
      " [  7610   2987]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_model = xgboost.XGBClassifier(max_depth=15, n_estimators=400, n_jobs=4, scale_pos_weight=2).fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
