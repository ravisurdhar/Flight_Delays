{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping & Adding Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original motivation for this project was to see if we can generate a model to predict flight delays based on weather, we need to add the departure and arrival airport weather at the originally scheduled time of departure and arrival. Luckily, Iowa State University's Mesonet service allows users to download historical airport weather data as .csv files.\n",
    "\n",
    "Rather than hammer their servers with ~10 million requests (two per row in our dataframe), I opted to download the weather for the entire year at each of the airports we're concerned with, then look up the weather for each flight ourself.\n",
    "\n",
    "We'll start by downloading the weather files we care about, using their python example script. This script takes a .csv of airports, so we'll create that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data from the previous notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 35)\n",
    "\n",
    "flights = pd.read_csv('flights_reduced.csv', \n",
    "                      index_col=0, \n",
    "                      dtype = {'AIRLINE': 'category', \n",
    "                                'ORIGIN_AIRPORT': 'category', \n",
    "                                'DESTINATION_AIRPORT': 'category', \n",
    "                                'DEPARTURE_DELAY': 'float32', \n",
    "                                'ARRIVAL_DELAY': 'float32', \n",
    "                                'DIVERTED': 'uint8', \n",
    "                                'CANCELLED': 'uint8', \n",
    "                                'CANCELLATION_REASON': 'category'}, \n",
    "                      parse_dates = ['SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL']).reset_index(drop=True)\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "all_airports = np.unique((np.append(flights['ORIGIN_AIRPORT'].unique(), flights['DESTINATION_AIRPORT'].unique())))\n",
    "\n",
    "# This is because Mesonet uses ICAO codes for airports outside of the CONUS (namely, Hawaii, \n",
    "# Alaska, and Puerto Rico) instead of IATA codes, and because Yuma's IATA code recently changed from YUM to NYL. \n",
    "all_airports = np.append(all_airports, ['PANC', 'PHTO', 'PHKO', 'PHLI', 'PHOG', 'TSJS', 'NYL'])\n",
    "all_airports[np.isin(all_airports, ['ANC', 'HNL', 'LIH', 'ITO', 'OGG', 'SJU', 'YUM'], invert=True)]\n",
    "\n",
    "import csv\n",
    "with open('all_airports.txt', 'w', newline='\\n') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    spamwriter.writerow(all_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Example script that scrapes data from the IEM ASOS download service\n",
    "Source: https://github.com/akrherz/iem/blob/master/scripts/asos/iem_scraper_example.py\n",
    "Utilizes this service: https://mesonet.agron.iastate.edu/request/download.phtml\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "# Python 2 and 3: alternative 4\n",
    "try:\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen\n",
    "\n",
    "# Number of attempts to download data\n",
    "MAX_ATTEMPTS = 6\n",
    "# HTTPS here can be problematic for installs that don't have Lets Encrypt CA\n",
    "SERVICE = \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "\n",
    "\n",
    "def download_data(uri):\n",
    "    \"\"\"Fetch the data from the IEM\n",
    "\n",
    "    The IEM download service has some protections in place to keep the number\n",
    "    of inbound requests in check.  This function implements an exponential\n",
    "    backoff to keep individual downloads from erroring.\n",
    "\n",
    "    Args:\n",
    "      uri (string): URL to fetch\n",
    "\n",
    "    Returns:\n",
    "      string data\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            data = urlopen(uri, timeout=300).read().decode('utf-8')\n",
    "            if data is not None and not data.startswith('ERROR'):\n",
    "                return data\n",
    "        except Exception as exp:\n",
    "            print(\"download_data(%s) failed with %s\" % (uri, exp))\n",
    "            time.sleep(5)\n",
    "        attempt += 1\n",
    "\n",
    "    print(\"Exhausted attempts to download, returning empty data\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_stations_from_filelist(filename):\n",
    "    \"\"\"Build a listing of stations from a simple file listing the stations.\n",
    "\n",
    "    The file should simply have one station per line.\n",
    "    \"\"\"\n",
    "    stations = []\n",
    "    for line in open(filename):\n",
    "        stations.append(line.strip())\n",
    "    return stations\n",
    "\n",
    "\n",
    "def get_stations_from_networks():\n",
    "    \"\"\"Build a station list by using a bunch of IEM networks.\"\"\"\n",
    "    stations = []\n",
    "    states = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n",
    "     MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n",
    "     WA WI WV WY\"\"\"\n",
    "    # IEM quirk to have Iowa AWOS sites in its own labeled network\n",
    "    networks = ['AWOS']\n",
    "    for state in states.split():\n",
    "        networks.append(\"%s_ASOS\" % (state,))\n",
    "\n",
    "    for network in networks:\n",
    "        # Get metadata\n",
    "        uri = (\"https://mesonet.agron.iastate.edu/\"\n",
    "               \"geojson/network/%s.geojson\") % (network,)\n",
    "        data = urlopen(uri)\n",
    "        jdict = json.load(data)\n",
    "        for site in jdict['features']:\n",
    "            stations.append(site['properties']['sid'])\n",
    "    return stations\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Our main method\"\"\"\n",
    "    # timestamps in UTC to request data for\n",
    "    startts = datetime.datetime(2015, 1, 1)\n",
    "    endts = datetime.datetime(2015, 12, 31)\n",
    "\n",
    "    service = SERVICE + \"data=sknt&data=p01i&data=vsby&data=gust&data=skyc1&data=skyc2&data=skyc3&data=skyl1&data=skyl2&data=skyl3&data=wxcodes&tz=Etc/UTC&format=onlycomma&latlon=no&\"\n",
    "\n",
    "    service += startts.strftime('year1=%Y&month1=%m&day1=%d&')\n",
    "    service += endts.strftime('year2=%Y&month2=%m&day2=%d&')\n",
    "\n",
    "    # Two examples of how to specify a list of stations\n",
    "    # stations = get_stations_from_networks()\n",
    "    stations = get_stations_from_filelist(\"Final Product Files//all_airports.txt\")\n",
    "    for station in stations:\n",
    "        uri = '%s&station=%s' % (service, station)\n",
    "        print('Downloading: %s' % (station, ))\n",
    "        data = download_data(uri)\n",
    "        outfn = 'Final Product Files//weather\\%s.csv' % (station)\n",
    "        out = open(outfn, 'w')\n",
    "        out.write(data)\n",
    "        out.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each flight's departure and arrival airport, we have to get the weather at the scheduled takeoff and landing times. Unfortunately, for ~5 million items, there's no fast way to do this. The following script appends Visibility (Miles), Cloud Ceiling (ft), Wind Speed (knots), and Hourly Precipitation (inches) for the Departure and Arrival airports of each flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Uncomment this line to only work on a portion of the flights\n",
    "#flights = flights.iloc[0:10000]\n",
    "\n",
    "# Create a list of all the airports in our flights dataframe\n",
    "all_airports = np.unique((np.append(flights['ORIGIN_AIRPORT'].unique(), flights['DESTINATION_AIRPORT'].unique())))\n",
    "\n",
    "# Create a dictionary where each key is a unique airport, and each value is a dataframe containing all the weather data for the \n",
    "# entire year at that airport.\n",
    "airport_wx_dict = {}\n",
    "for i in all_airports:\n",
    "    i_wx = pd.read_csv('weather\\\\%s.csv' % i, \n",
    "        parse_dates=['valid'],\n",
    "        dtype= {'station' : 'category',\n",
    "               '  skyc1 ': 'category',\n",
    "               '  skyc2 ': 'category',\n",
    "               '  skyc3 ': 'category',\n",
    "               '  wxcodes': 'category'}\n",
    "        ).rename(columns=lambda x: x.strip())\n",
    "    airport_wx_dict[i] = i_wx\n",
    "    \n",
    "   \n",
    "def get_wx(airport, dt):\n",
    "    \n",
    "    # For a given airport and datetime, find the weather\n",
    "    smallest_delta = datetime.timedelta(days=365)\n",
    "    closest_time = 0\n",
    "    airport_wx_df = airport_wx_dict[airport]\n",
    "    for i in airport_wx_df['valid']:\n",
    "        delta = abs(i-dt)\n",
    "        if delta < smallest_delta: \n",
    "            smallest_delta = delta\n",
    "            closest_time = i\n",
    "        if delta > smallest_delta:\n",
    "            break\n",
    "    airport_wx = airport_wx_df[airport_wx_df['valid'] == closest_time]\n",
    "    \n",
    "    # Get the cloud ceiling (defined as the lowest Overcast or Broken cloud layer)\n",
    "    if (airport_wx['skyc1'][0] == 'OVC') or (airport_wx['skyc1'][0] == 'BKN'):\n",
    "        ceiling = pd.to_numeric(airport_wx['skyl1'], downcast='unsigned')\n",
    "    elif airport_wx['skyc2'][0] == 'OVC' or airport_wx['skyc2'][0] == 'BKN':\n",
    "        ceiling = pd.to_numeric(airport_wx['skyl2'], downcast='unsigned')\n",
    "    elif airport_wx['skyc3'][0] == 'OVC' or airport_wx['skyc3'][0] == 'BKN':\n",
    "        ceiling = pd.to_numeric(airport_wx['skyl3'], downcast='unsigned')\n",
    "    else: ceiling = 25000\n",
    "    \n",
    "    # set missing wind values ('M') to 0 knots\n",
    "    try:\n",
    "        wind = pd.to_numeric(airport_wx['sknt'], downcast='unsigned')\n",
    "    except:\n",
    "        wind = 0  \n",
    "    \n",
    "    # set missing visibility values to 10 miles\n",
    "    try:\n",
    "        visibility = pd.to_numeric(airport_wx['vsby'], downcast='unsigned')\n",
    "    except:\n",
    "        visibility = 10\n",
    "    \n",
    "    # set missing precipitation values to 0 inches\n",
    "    try:\n",
    "        precip = pd.to_numeric(airport_wx['p01i'], downcast='unsigned')\n",
    "    except: \n",
    "          precip=0\n",
    "    \n",
    "    return pd.DataFrame({'CEILING' : ceiling, 'VISIBILITY': visibility, 'WIND_SPEED': wind, 'PRECIPITATION': precip})    \n",
    "\n",
    "def get_2wx(df):\n",
    "    # for a single flight, return a Series containing the origin and destination weather\n",
    "    origin_airport = df['ORIGIN_AIRPORT']\n",
    "    scheduled_departure = df['SCHEDULED_DEPARTURE']\n",
    "    destination_airport = df['DESTINATION_AIRPORT']\n",
    "    scheduled_arrival = df['SCHEDULED_ARRIVAL']\n",
    "    origin_wx = get_wx(origin_airport, scheduled_departure).add_prefix('ORIGIN_').reset_index(drop=True)\n",
    "    destination_wx = get_wx(destination_airport, scheduled_arrival).add_prefix('DESTINATION_').reset_index(drop=True)\n",
    "    return pd.concat([origin_wx, destination_wx], axis=1)\n",
    "\n",
    "# Get the weather for the entire flights dataframe\n",
    "wx_data = flights.apply(lambda x: get_2wx(x), axis = 1)\n",
    "\n",
    "# Convert weather data to a DataFrame\n",
    "wx_df = pd.DataFrame()\n",
    "for i in wx_data: \n",
    "    wx_df = wx_df.append(i)\n",
    "wx_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the flight and weather dataframes\n",
    "flights.reset_index(drop=True, inplace=True)\n",
    "flights_with_wx = pd.concat([flights, wx_df], axis=1)\n",
    "flights_with_wx.to_csv('flights_with_wx.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
